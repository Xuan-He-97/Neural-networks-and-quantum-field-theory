{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural networks and quantum field theory.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMHT1eAkDE0KmwiBz7vkMZ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xuan-He-97/Neural-networks-and-quantum-field-theory/blob/main/Neural_networks_and_quantum_field_theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_iP5moDhWGA"
      },
      "source": [
        "## Gaussian Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmBwxrwshX3T"
      },
      "source": [
        "### Concept of Gaussian Process\n",
        "\n",
        "Prior over functions: $\\; p(f)$\n",
        "\n",
        "For training: minimize negative log marginal likelihood $\\mathcal{L}(\\theta)\\;$\n",
        "($\\theta$ is hyperparameters and noise level)\n",
        "\n",
        "Can we use Beyesian regression? $\\;\\;\\;p(f|\\mathcal{D}) = \\frac{p(f)p(\\mathcal{D}|f)}{P(\\mathcal{D})}$\n",
        "\n",
        "<br>\n",
        "\n",
        "Gaussian process is a set of random variables $f$, indexed by a continuous variable $\\;\\; f = f(x)$. Any subset of finite function variables $\\{f_n\\}^N_{n=1}$ has joint (zero mean) Gaussian distribution.\n",
        "\n",
        "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; p(\\textbf{f} | \\textbf{X}) = \\mathcal{N}(0, \\textbf{K})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d3BwX_I3pln"
      },
      "source": [
        "infinite width $\\;\\;\\;\\;\\;\\;\\;\\Longleftrightarrow\\;\\;\\;\\;\\;\\;\\;$ GP $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\Longleftrightarrow\\;\\;\\;\\;\\;\\;\\;$ free field theory\n",
        "\n",
        "finite width $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\Longleftrightarrow\\;\\;\\;\\;\\;\\;\\;$ NGP $\\;\\;\\;\\;\\;\\;\\;\\Longleftrightarrow\\;\\;\\;\\;\\;\\;\\;$ effective field theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhFpEbO185XH"
      },
      "source": [
        "Function over $x\\;$: $\\;f(x)$ \n",
        "\n",
        "$\\;f(x)$ has a distribution: $p(f)$\n",
        "\n",
        "$x$ is like time/space coordinates. Infinite choices of $x$, but for any finite subset of $x$, the finite function variables $\\{f_i\\}^m_{i=1}$ has joint (zero mean) Gaussian distribution.\n",
        "\n",
        "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; p(\\textbf{f} | \\textbf{X}) \\sim \\mathcal{N}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buQQmpaX_fmV"
      },
      "source": [
        "For neural networks, $x$ is our input data, $f$ is the network. \n",
        "\n",
        "$f_{\\theta, N}:$ $R^{d_{in}}$ $\\to R^{d_{out}}$\n",
        "\n",
        "parameter space distribution $\\to$ function space distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P3c31L4ePBB"
      },
      "source": [
        "### Neural Network Corresponds to Gaussian Process\n",
        "\n",
        "parameter space distribution $\\to$ function space distribution\n",
        "\n",
        "$f_{\\theta, N}:$ $R^{d_{in}}$ $\\to R^{d_{out}}$\n",
        "\n",
        "Consider one hidden later of $N$ units, and output dimension is $1$:\n",
        "\n",
        "$\\;\\;\\;\\;\\;\\; f(\\textbf{x}) = b_1 + \\displaystyle \\sum^{N}_{j=1} W_1^j \\sigma(\\textbf{x}; \\textbf{W}_0^j, b_0^j)$\n",
        "\n",
        "$\\textbf{W}_0^j$ is i.i.d., $b_0^j$, $b_1$ and $W_1^j$ is zero mean and independent.\n",
        "\n",
        "In this paper, $b_0^j, b_1 \\sim \\mathcal{N}(0, \\sigma_b^2)\\;$, $\\;\\textbf{W}_0^j \\sim \\mathcal{N}(0, \\sigma_W^2 / d_{in})\\;$, $\\; \\textbf{W}_1^j \\sim \\mathcal{N}(0, \\sigma_W^2/N)$ \n",
        "\n",
        "(normalize w.r.t. input dimension)\n",
        "\n",
        "kernel function dependents on the activation function.\n",
        "\n",
        "- For neural networks with $N \\to \\infty$:  Gaussian distribution on function space (Gaussian Process) (distribution depends on the GP kernel)\n",
        "\n",
        "- For neural networks with $N < \\infty$: distribution receives $1/N$ corrections $\\to$ non-Gaussian Process (NGP)\n",
        "\n",
        "Learning $\\to$ data-induced flow of function space distribution (remain NGP during training) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG09WMgUFUZL"
      },
      "source": [
        "### NN-QFT correspondence\n",
        "\n",
        "EFT approach can be used for any architecture admitting a GP limit.\n",
        "\n",
        "Admit GP limits: \n",
        "\n",
        "- multipayer perceptrons, recurrent NN, skip connections,, convolutions, graph convolutions, pooling, batch/layer normalization, attention.\n",
        "\n",
        "- both randomly initialized NN or appropriately trained networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PJmjxh9WyYJ"
      },
      "source": [
        "### Meaning of NN-QFT Correspondence\n",
        "\n",
        "$P[f] \\sim e^{-S}$\n",
        "\n",
        "overparameterization (increasingly large numbers of parameters) $\\Leftrightarrow$ neural network likelihood simplicity (simple distributions)\n",
        "\n",
        "because of:\n",
        "\n",
        "- asymptotic NN $\\to$ GP\n",
        "\n",
        "- non-Gaussian correction is $1/N$ (actually need only a single number to correct correlation functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhdkTLndkRk5"
      },
      "source": [
        "## Asymptotic NN and Free Field Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv2c-muKch9s"
      },
      "source": [
        "### GP/asymptotic NN and Free QFT correspondence\n",
        "\n",
        "| GP/asymptotic NN | Free QFT |\n",
        "| --- | --- |\n",
        "| Input x | External space or momentum space point |\n",
        "| Kernel $K(x_1, x_2)$ | Feynman propagator |\n",
        "| Asymptotic NN $f(x)$ | Free Field |\n",
        "| Log-likelihood | Free action $S_{GP}$ |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKLLsj9NlCdO"
      },
      "source": [
        "### GP/asymptotic NN\n",
        "\n",
        "$\\theta \\sim P(\\theta)$ + network architecture $\\to$ $P(f)$\n",
        "\n",
        "$\\{f(x_1), ..., f(x_k)\\} \\sim \\mathcal{N}(\\mu, \\Xi^{-1})$ \n",
        "\n",
        "assumption: $\\mu = 0$\n",
        "\n",
        "$(\\Xi^{-1})_{ij} = K_{ij}$\n",
        "\n",
        "- Correlation function (n-pt functions)\n",
        "\n",
        "$G^{(n)}(x_1, ..., x_n) = \\frac{\\int df f_1 ... f_n e^{- S}}{Z}$\n",
        "\n",
        "- partition function $Z = \\int df e^{-S}$\n",
        "\n",
        "- discrete action $S = \\frac{1}{2} f_i \\Xi_{ij} f_j$ (Einstein summation)\n",
        "\n",
        "- continuous action $S = \\frac{1}{2} \\int d^{d_{in}}x d^{d_{in}}x' f(x) \\Xi(x, x')f(x')$\n",
        "\n",
        "- inverse covariance function $\\int d^{d_{in}}x' K(x, x') \\Xi(x', x'') = \\delta^{(d_{in})} (x - x'')$\n",
        "\n",
        "- local GP $S = \\frac{1}{2} \\int d^{d_{in}}x f(x) \\Xi(x) f(x)$  where $\\Xi(x, x') = \\Xi(x) L_\\sigma(x, x')$ and $L_0(x, x') = \\delta^{(d_{in})} (x - x')$\n",
        "\n",
        "- ultra-local GP: f is constant $S = \\frac{1}{2} f \\Xi f$ where $K\\Xi = 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSj2zCmf03jV"
      },
      "source": [
        "question: what are propagator (probability or amplitude of propagation of a particle from one point to another) and action in QFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG7tOClX1V7S"
      },
      "source": [
        "### Free Field Theory\n",
        "\n",
        "- quantum field $\\phi(x)$\n",
        "\n",
        "- path integral $Z = \\int D \\phi e^{-S[\\phi]}$\n",
        "\n",
        "- action $S[\\phi] = \\int d^dx \\phi(x) (\\Box + m^2) \\phi(x)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZAj46CFPjQ0"
      },
      "source": [
        "question A.21"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g6t7KTxPn08"
      },
      "source": [
        "### Computing correlation function\n",
        "\n",
        "$G_{GP}^{(n)}(x_1, ..., x_n) = \\sum_{p\\in \\textrm{Wick}(x_1, ..., x_n)} K(a_1, b_1)...K(a_{n/2}, b_{n/2})$\n",
        "(can be derived)\n",
        "\n",
        "Diagrammatic representaions: pairs of points, due to the Gaussian nature of $Z_{GP}$\n",
        "\n",
        "Question: how to understand"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzQRH2JWPn3b"
      },
      "source": [
        "### GP in neural networks\n",
        "\n",
        "- Why GP: by CLT\n",
        "\n",
        "$f_{\\theta, N}(x) = z_1^k = \\sum_{j=1}^N W_1^{jk}x_1^j + b_1^k$\n",
        "\n",
        "<br>\n",
        "\n",
        "- $f$ has two parts\n",
        "\n",
        "$f = f_b \\text{(ultra-local)} + f_W \\text{(depends on model and activation)}$ $\\to$ $G^{(2)}(x_1, x_2) = G_b^{(2)}(x_1, x_2) + G_W^{(2)}(x_1, x_2)$\n",
        "\n",
        "$G^{(4)}(x_1, x_2, x_3, x_4) = G_b^{(4)}(x_1, x_2, x_3, x_4) + G_W^{(4)}(x_1, x_2, x_3, x_4) + \\sum_{i \\ne j \\ne k \\ne l} G_b^{(2)}(x_i, x_j) G_W^{(2)}(x_k, x_l)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5VySV7LPn8b"
      },
      "source": [
        "TODO: kernel derivation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp4-mkF_Pn-7"
      },
      "source": [
        "### Three networks\n",
        "\n",
        "- Erf-net\n",
        "\n",
        "- ReLu-net\n",
        "\n",
        "- Gauss-net (translation invariant kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG2Uh9Xyh9CH"
      },
      "source": [
        "## Experiments of GP limit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW76Aj3oiDpf"
      },
      "source": [
        "### How to measure\n",
        "\n",
        "- normalized deviation $m_n$\n",
        "\n",
        "$m_n(x_1, ..., x_n) = \\Delta G^{(n)}(x_1, ..., x_n) / G_{GP}^{(n)}(x_1, ..., x_n) $\n",
        "\n",
        "$\\Delta G^{(n)}(x_1, ..., x_n) = G^{(n)}(x_1, ..., x_n) - G_{GP}^{(n)}(x_1, ..., x_n)$\n",
        "\n",
        "<br>\n",
        "\n",
        "$G^{(n)}(x_1, ..., x_n) = \\mathcal{E}[f_{\\alpha}(x_1)...f_{\\alpha}(x_n)]$ is measured in the experiment $100 \\text{(experiments)} * 10^5 \\text{(nets)}$ times \n",
        "\n",
        "weights and biases is drawn from Gaussian dist. with mean equals zero and std equals 1\n",
        "\n",
        "<br>\n",
        "\n",
        "$G_{GP}^{(n)}(x_1, ..., x_n)$ is computed using Wick contraction\n",
        "\n",
        "<br>\n",
        "\n",
        "$d_{in} = d_{out} = 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5acd_bXPiDzu"
      },
      "source": [
        "### Inputs\n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/79208856/119465386-33560c80-bd76-11eb-876a-5d61c736fbd9.png)\n",
        "\n",
        "- inputs of ReLu-net and Erf-net are chosen to be positive so that the kernel is always positive\n",
        "\n",
        "- inputs are chosen where the finite width NGP is well approximated by local-operator correction terms to the associated log-likelihood,\n",
        "\n",
        "question local-operator correction terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HusKxH7DiD1m"
      },
      "source": [
        "### Results\n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/79208856/119467356-03a80400-bd78-11eb-86c1-87690245757c.png)\n",
        "\n",
        "- 2-pt function is exactly the kernel even away from GP (can be proved)\n",
        "\n",
        "- background is average std across 100 experiments of $m_n$\n",
        "\n",
        "- $\\Delta G^{(n)} \\propto N^{-1}$ for $n = 4, 6$  (can be proved)\n",
        "\n",
        "- connected contribution of the 4-pt function is same as $\\Delta G^{(4)}$ $\\propto N^{-1}$  (can be proved)\n",
        "\n",
        "- connected contribution of the 6-pt function is $\\propto N^{-2}$  (can be proved)\n",
        "\n",
        "- $G^{(2k)}(x_1, ..., x_{2k})|_{\\text{connected}} = \\left[ G^{(2k)}(x_1, ..., x_{2k}) - S(x_1, ..., x_{2k}) \\right]|_{\\text{internal indices same}}$\n",
        "\n",
        "- $G^{(2k)}(x_1, ..., x_{2k})|_{\\text{connected}} \\propto \\frac{1}{N^{k-1}}$\n",
        "\n",
        "- $G^{(2k)}(x_1, ..., x_{2k})|_{\\text{connected}}$ is computed from expectation of layer postactivation value\n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/79208856/119483310-68b72600-bd87-11eb-968c-ff39dbc32e92.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzio1TPgl6ow"
      },
      "source": [
        "$\\Delta G^{(2)} \\propto 0$\n",
        "\n",
        "$\\Delta G^{(4)} \\propto N^{-1}$\n",
        "\n",
        "$\\Delta G^{(6)} \\propto N^{-1} + N^{-2} \\propto N^{-1}$\n",
        "\n",
        "$G^{(4)}(x_1, ..., x_{4})|_{\\text{connected}} = \\Delta G^{(4)} \\propto N^{-1}$\n",
        "\n",
        "$G^{(6)}(x_1, ..., x_{6})|_{\\text{connected}} = \\Delta G^{(6)} - \\sum G^{(4)}(x_1, ..., x_{4})|_{\\text{connected}} \\; G^{(2)}(x_5, x_6)  \\propto N^{-2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggvwvkuYiD3h"
      },
      "source": [
        "## Non-Gaussian processes with effective field theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh3go-aBU87Z"
      },
      "source": [
        "### NGP NN and EFT correspondence\n",
        "\n",
        "| NGP NN | EFT |\n",
        "| --- | --- |\n",
        "| Input x | External space or momentum space point |\n",
        "| Kernel $K(x_1, x_2)$ | Free or exact propagator |\n",
        "| NN output $f(x)$ | Interacting Field |\n",
        "| Non-Gaussianities | Interactions |\n",
        "| Non-Gaussian coefficients | Coupling strengths |\n",
        "| Log-likelihood | EFT action $S$ |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLbmKnCdiD5Z"
      },
      "source": [
        "### EFT: scales\n",
        "\n",
        "$[S_{GP}] = 0 \\to [f] = - \\frac{2d_{in} + [\\Xi]}{2}$\n",
        "\n",
        "for $\\mathcal{O}_k := g_k f(x)^k$ in $\\Delta S $ as $ \\int d^{d_{in}}x \\mathcal{O}_k$\n",
        "\n",
        "$[g_k] = -d_{in} + \\frac{k(2d_{in} + [\\Xi])}{2} =  -d_{in} - \\frac{k([K])}{2} $\n",
        "\n",
        "$[\\Xi] + [K] = -2d_{in}$\n",
        "\n",
        "$\\mathcal{O}_k$ can be ignored for sufficiently large $k$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZkAlOhGsBJg"
      },
      "source": [
        "### How to construct the effective action of an NGP\n",
        "\n",
        "- Determine the symmetries respected by the system of interest.\n",
        "\n",
        "- Fix and upper bound $k$ on the dimentison of any operator appearing in $\\Delta S$.\n",
        "\n",
        "- Define $\\Delta S$ to contrain all operators of dimension $\\leqslant k $ that respect the symmetries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79mXqssPvKsP"
      },
      "source": [
        "### Perturbation Theory\n",
        "\n",
        "$G^{(n)}(x_1, ..., x_n) = \\frac{\\int df f_1 ... f_n e^{- S}}{Z_0}$\n",
        "\n",
        "$Z_0 = \\int df e^{-S}$\n",
        "\n",
        "$S = S_{GP} + \\Delta S$\n",
        "\n",
        "$S_{GP}$ is Gaussian so the first part is computable, but $\\Delta S$ is not computable.\n",
        "\n",
        "But $\\Delta S$ is small $\\to$ perturbation theory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCDGZYfzyMhT"
      },
      "source": [
        "### Cutoffs\n",
        "\n",
        "$S \\to S_{\\Lambda}$ to deal with divergences. \n",
        "\n",
        "should be insensitive to the choice of $\\Lambda$ $\\to$ obey RGEs (ReLu-net satisfies)\n",
        "\n",
        "$\\Delta S = \\int d^{d_{in}}x \\left[ \\lambda f(x)^4 + \\kappa f(x)^6 \\right]$\n",
        "\n",
        "question: why $k \\geqslant 4$\n",
        "\n",
        "- odd-order coeefficients are zero. Odd-point function must be zero because means of weights and biases are zero or $S$ must have $f \\to -f$ symmetry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQygKTXaV76Z"
      },
      "source": [
        "### Represent correlation function diagrammatically - Feynman rules\n",
        "\n",
        "$O(\\lambda^l \\kappa^m)$ correction to $G^{(n)}$, $l$ 4-pt interaction vertices and $m$ 6-pt interaction vertices.\n",
        "\n",
        "$G^{(2)}(x_1, x_2) = K(x_1, x_2) \\; \\to \\; S = S_{G} + \\Delta S $ \n",
        "\n",
        "$G^{(n)}(x_1, ..., x_n) = \\frac{\\int df f(x_1)...f(x_n) \\left[ 1 - \\int d^{d_{in}}x g_k f(x)^k + O(g_k^2) \\right] e^{-S_{GP}} / Z_{GP, 0}}{\\int df \\left[ 1 - \\int d^{d_{in}}x g_k f(x)^k + O(g_k^2) \\right] e^{-S_{GP}} / Z_{GP, 0}} $\n",
        "\n",
        "<br>\n",
        "\n",
        "- Feynman rules\n",
        "![alt text](https://user-images.githubusercontent.com/79208856/119619696-d706f180-be36-11eb-8729-43821cd96c78.png)\n",
        "\n",
        "- Feynman lines from --- to - - $\\Leftrightarrow$ propagator of $S_G \\to $ 2-pt function\n",
        "\n",
        "- Expand to leading order in $\\lambda$ and $\\kappa$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFz8sqIscZiW"
      },
      "source": [
        "question eq.70 why linear? Taylor expansion?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmiX3w1AcuNs"
      },
      "source": [
        "### Symmetries and Interaction strength coefficient (also called coupling constants in QFT)\n",
        "\n",
        "- Technical naturalness: a coupling $g$ appearing in $\\Delta S$ may be small relative to $\\Lambda$ if a symmetry is restored when $g$ is set to zero.\n",
        "\n",
        "$\\Delta g = 0$ if $S$ has a symmetry.\n",
        "\n",
        "$g \\to 0 \\Rightarrow \\Delta g \\to 0$\n",
        "\n",
        "<br>\n",
        "\n",
        "- Technical naturalness for NGP neural networks:\n",
        "\n",
        "$\\lambda (x) = \\bar{\\lambda} + \\delta \\lambda (x)$\n",
        "\n",
        "if there is symmetry $T$ in $\\delta \\lambda \\to 0$ limit\n",
        "\n",
        "then $\\frac{\\delta \\lambda}{\\lambda} \\leqslant 1$\n",
        "\n",
        "which means the couplings in NGPs are (near) constants. (not proven but can be tested: coupling are effectively constants in Gauss-net)\n",
        "\n",
        "$T$ : $K(x, y)$ is translation invariant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6R7wz4x4XOz"
      },
      "source": [
        "$\\lambda, \\; \\kappa $ is constants by symmetry at GP and Technical Naturalness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzD2-UBcCFSM"
      },
      "source": [
        "### Independence of single-layer networks\n",
        "\n",
        "$S_{GP} = S_{GP}^b + S_{GP}^W = \\frac{1}{2} \\int d^{d_{in}}x d^{d_{in}}y \\left[ f_b(x) \\Xi_b (x, y) f_b(y) + f_W(x) \\Xi_W (x, y) f_W(y) \\right]$\n",
        "\n",
        "$K(x, y) = K_b(x, y) + K_W(x, y)$\n",
        "\n",
        "$\\Delta S = \\Delta S_b + \\Delta S_W = \\Delta S_W$\n",
        "\n",
        "similarly $G^{(n)}(x_1, ..., x_n) = \\frac{\\int df f(x_1)...f(x_n) \\left[ 1 - \\int d^{d_{in}}x g_k f_W(x)^k + O(g_k^2) \\right] e^{-S_{GP}} / Z_{GP, 0}}{\\int df \\left[ 1 - \\int d^{d_{in}}x g_k f_W(x)^k + O(g_k^2) \\right] e^{-S_{GP}} / Z_{GP, 0}} \\; \\Rightarrow \\; $ Feynman diagrams\n",
        "\n",
        "dashed lines in Fetnman diagram correspond to $K_W(u, v)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfPYddRSPQ58"
      },
      "source": [
        "## Experiments of NGP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBDJuGorPXX5"
      },
      "source": [
        "### compute coupling constants\n",
        "\n",
        "- if $\\lambda$ is constant\n",
        "\n",
        "&emsp; $\\lambda = \\frac{K(x_1, x_2)K(x_3, x_4) + K(x_1, x_3)K(x_2, x_4) + K(x_1, x_4)K(x_2, x_3) - G^{(4)}(x_1, x_2, x_3, x_4)}{24 \\int d^{d_{in}}y K_W(x_1, y)K_W(x_2, y)K_W(x_3, y)K_W(x_4, y)}$\n",
        "\n",
        "- if $\\lambda$ is not constant\n",
        "\n",
        "&emsp; $\\lambda (y) = \\bar{\\lambda} + \\delta \\lambda (y)$\n",
        "\n",
        "&emsp; $\\bar{\\lambda} = \\frac{K(x_1, x_2)K(x_3, x_4) + K(x_1, x_3)K(x_2, x_4) + K(x_1, x_4)K(x_2, x_3) - G^{(4)} \\; (x_1, x_2, x_3, x_4)}{24 \\int d^{d_{in}}y \\; \\Delta_{1234y}} - \\frac{\\int d^{d_{in}}y \\; \\delta \\lambda (y) \\; \\Delta_{1234y}}{\\int d^{d_{in}}y \\; \\Delta_{1234y}}$\n",
        "\n",
        "&emsp;  Where $\\Delta_{1234y} = K_W(x_1, y)K_W(x_2, y)K_W(x_3, y)K_W(x_4, y)$\n",
        "\n",
        "- if $\\delta \\lambda (y)$ is small \n",
        "\n",
        "&emsp; $\\lambda \\backsimeq \\bar{\\lambda} \\backsimeq \\text{mean} (\\lambda_m(x_1, x_2, x_3, x_4))$\n",
        "\n",
        "- for $\\kappa$\n",
        "\n",
        "&emsp; $\\delta'(x_1, ..., x_6) := G^{(6)} \\; (x_1, ..., x_6) - \\sum_{\\text{15 combinations}} \\left[ K(x_i, x_j)K(x_k, x_l)K(x_i, x_j) - 24 \\int d^{d_{in}}y \\lambda K_W(x_i, y)K_W(x_j, y)K_W(x_k, y)K_W(x_l, y)K_W(x_m, x_n) \\right] \\;$\n",
        "\n",
        "&emsp; $\\delta(x_1, ..., x_6) := \\frac{\\delta'(x_1, ..., x_6)}{G^{(6)} \\; (x_1, ..., x_6)} $\n",
        "\n",
        "&emsp; $\\delta(x_1, ..., x_6) \\to 0$ in large $N$ and large $\\Lambda$ limits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QrUnSvXWxDw"
      },
      "source": [
        "### Results\n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/79208856/119772490-d3847080-bef1-11eb-861d-961438b21d71.png)\n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/79208856/119772541-f020a880-bef1-11eb-806b-ff150dfdbd2a.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orFWVZYX0uuC"
      },
      "source": [
        "## Experiments of fitting EFT parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0drCYKd-1WRZ"
      },
      "source": [
        "### Compute deviation from GP in 4-pt function\n",
        "\n",
        "$\\Delta S_W = \\int d^{d_{in}}x \\lambda (x) f_W(x)^4$\n",
        "\n",
        "design three functional forms of $\\lambda$ \n",
        "\n",
        "$\\Delta G^{(4)}_{EFT} = \\lambda_0 T_0(x1, .., x_4) + \\lambda_2 T_2(x1, .., x_4) + \\lambda_{NL} T_{NL}(x1, .., x_4)$\n",
        "\n",
        "- $T_0(x1, .., x_4) = 24 \\int d^{d_{in}}x K_W(x_1, x)K_W(x_2, x)K_W(x_3, x)K_W(x_4, x)$\n",
        "\n",
        "- $T_2(x1, .., x_4) = 24 \\int d^{d_{in}}x x^2 K_W(x_1, x)K_W(x_2, x)K_W(x_3, x)K_W(x_4, x)$\n",
        "\n",
        "- nonlocal term: $T_{NL}(x1, .., x_4) = 8 \\int d^{d_{in}}x d^{d_{in}}y \\left[ K_W(x_1, x)K_W(x_2, x)K_W(x_3, y)K_W(x_4, y) + K_W(x_1, x)K_W(x_2, y)K_W(x_3, x)K_W(x_4, y) + K_W(x_1, x)K_W(x_2, y)K_W(x_3, y)K_W(x_4, x) \\right]$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCVKNzt6yW0C"
      },
      "source": [
        "\n",
        "Predict $\\Delta G^{(4)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQbXaLYP6GZq"
      },
      "source": [
        "### Experimental measurements of deviation\n",
        "\n",
        "$\\Delta G^{(4)}_{exp}$ is averaged over $10^7$ experiments\n",
        "\n",
        "thus we can find values of $\\lambda$ to minimize the mean squared error between $\\Delta G^{(4)}_{exp}$ and $\\Delta G^{(4)}_{EFT}$\n",
        "\n",
        "test effectiveness of measurement value by making predictions for the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuXlc8ddB5_B"
      },
      "source": [
        "## Wilsonian Renormalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiPPzpV-1WTv"
      },
      "source": [
        "### How couplings change with $\\Lambda$\n",
        "\n",
        "- goal of RG: solve divergences arising from integrals over the space of inputs\n",
        "\n",
        "using cutoff: $\\Delta S_{\\Lambda} = \\int^{\\Lambda}_{-\\Lambda} d^{d_{in}}x \\sum_{l \\leqslant k} g_{\\mathcal{O}_l}(\\Lambda)\\mathcal{O}_l$\n",
        "\n",
        "- How to understand cutoff: MNIST maximum brightness and darkness scale.\n",
        "\n",
        "- We can extract values of $g_{\\mathcal{O}_l}(\\Lambda)$ from experimental result of $G^{(n)}(x_1, ... , x_n)$\n",
        "\n",
        "- $\\Delta S_{\\Lambda}$ should satisfy $\\frac{d G^{(n)}\\;(x_1, ... , x_n)}{d \\log \\Lambda} = 0$\n",
        "\n",
        "- $\\beta$ -function: $\\beta(g_{\\mathcal{O}_l}) := \\frac{d(g_{\\mathcal{O}_l}(\\Lambda))}{d \\log \\Lambda}$\n",
        "\n",
        "- RG flow: flow in the couplings induced by varying $\\Lambda$\n",
        "\n",
        "- Along a direction of flow, couplings can be irrelevant (decrease), relevant (increase), marginal (same). \n",
        "\n",
        "- Since $[\\lambda] = -d_{in} - 2 [K]$, $\\;\\;\\;$ $[\\kappa] = -d_{in} - 3[K]$. When $[K] > 0$, $\\kappa$ decrease more quickly than $\\lambda$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pogwO662g8zs"
      },
      "source": [
        "### $\\beta$ - functions\n",
        "\n",
        "- Kernels have model independent term and model dependent term\n",
        "\n",
        "$K(x, x')=\\alpha + \\varsigma(x, x')$\n",
        "\n",
        "<br>\n",
        "\n",
        "- Thus correlation functions can be written in combination of model independent and dependent terms\n",
        "\n",
        "$ G^{(4)}\\;(x_1, ... , x_4) = \\gamma_{4, 0} + \\varrho_{4, 0} - \\int^{\\Lambda}_{-\\Lambda} d^{d_{in}}x (\\gamma_{4, \\lambda} + \\varrho_{4, \\lambda}) - \\kappa \\int^{\\Lambda}_{-\\Lambda} d^{d_{in}}x (\\gamma_{4, \\kappa} + \\varrho_{4, \\kappa})$\n",
        "\n",
        "$ G^{(6)}\\;(x_1, ... , x_6) = \\gamma_{6, 0} + \\varrho_{6, 0} - \\int^{\\Lambda}_{-\\Lambda} d^{d_{in}}x (\\gamma_{6, \\lambda} + \\varrho_{6, \\lambda}) - \\kappa \\int^{\\Lambda}_{-\\Lambda} d^{d_{in}}x (\\gamma_{6, \\kappa} + \\varrho_{6, \\kappa})$\n",
        "\n",
        "<br>\n",
        "\n",
        "- take derivative and cancel out the last term ($\\kappa$ is negligible)\n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/79208856/120148346-a51cd300-c21a-11eb-92c3-ce00cd3d4d9c.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "428J6FDpICMl"
      },
      "source": [
        "### RG in neural networks\n",
        "\n",
        "- Because bias term is Gaussian\n",
        "\n",
        "$\\Delta S_{W, \\Lambda} = \\int^{\\Lambda}_{-\\Lambda} d^{d_{in}}x \\displaystyle \\sum_{l \\leqslant k} g_{\\mathcal{O}_l}(\\Lambda)\\mathcal{O}_l$\n",
        "\n",
        "$[\\lambda] = -d_{in} - 2 [K_W]$, $\\;\\;\\;$ $[\\kappa] = -d_{in} - 3[K_W]$. \n",
        "\n",
        "<br>\n",
        "\n",
        "- For network architecture\n",
        "\n",
        "$K_W(x, x') = \\varsigma(x, x')$\n",
        "\n",
        "$ G^{(4)}\\;(x_1, ... , x_4) = \\gamma_{4, 0} + \\varrho_{4, 0} - \\int^{\\Lambda}_{-\\Lambda} d^{d_{in}}x \\varrho_{4, \\lambda} - \\kappa \\int^{\\Lambda}_{-\\Lambda} d^{d_{in}}x \\varrho_{4, \\kappa}$\n",
        "\n",
        "$ G^{(6)}\\;(x_1, ... , x_6) = \\gamma_{6, 0} + \\varrho_{6, 0} - \\int^{\\Lambda}_{-\\Lambda} d^{d_{in}}x \\varrho_{6, \\lambda} - \\kappa \\int^{\\Lambda}_{-\\Lambda} d^{d_{in}}x \\varrho_{6, \\kappa}$\n",
        "\n",
        "<br>\n",
        "\n",
        "- RG equations\n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/79208856/120154133-1ca23080-c222-11eb-8f34-8e651984f7bd.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-sl2KhGQi_F"
      },
      "source": [
        "### RG flows in Gauss-net\n",
        "\n",
        "$K_{\\text{Gauss}}(x, x') = \\sigma_b^2 + \\sigma_W^2 \\exp{\\left[ - \\frac{\\sigma_W^2 \\; |x-x'|^2}{2d_{in}}\\right]}$\n",
        "\n",
        "- $K_{\\text{Gauss}}$ converges for large integration value, thus $\\Lambda \\to \\infty$\n",
        "\n",
        "- for small $x$ and $x'$, $[K_W]<0$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FISjXOa1m5O0"
      },
      "source": [
        "### RuLU-net\n",
        "\n",
        "$K_W(x, x') = \\varsigma(x, x') = h_1(x, x')h_2(\\theta)$\n",
        "\n",
        "$h_1(x, x') = \\sqrt{(\\sigma_b^2 + \\frac{\\sigma_W^2}{d_{in}} x \\cdot x)(\\sigma_b^2 + \\frac{\\sigma_W^2}{d_{in}} x' \\cdot x')}$\n",
        "\n",
        "$h_2(\\theta) = (sin \\theta + (\\pi - \\theta) cos \\theta)$\n",
        "\n",
        "- $[K_W(x, x')] = 2$\n",
        "\n",
        "- $[\\lambda] = -d_{in} - 4, \\; [\\kappa] = -d_{in} - 6$\n",
        "\n",
        "![alt text](https://user-images.githubusercontent.com/79208856/120420985-074f1280-c398-11eb-92d2-edc0fc4e10b5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DZ9VYD9gRJp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Eoc3LJiC7G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}